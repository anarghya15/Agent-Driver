{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/agentdriver/lib/python3.10/site-packages/gdown/__main__.py:140: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
      "  warnings.warn(\n",
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1i-JXtbOsvXuKyT1sWKX9utjpRuo1isrW\n",
      "From (redirected): https://drive.google.com/uc?id=1i-JXtbOsvXuKyT1sWKX9utjpRuo1isrW&confirm=t&uuid=c0fab215-2d3c-4fb3-8b09-bab3c406bf99\n",
      "To: /home/ubuntu/Ollama/Agent-Driver/data/finetune.zip\n",
      "100%|██████████████████████████████████████| 6.91M/6.91M [00:00<00:00, 8.45MB/s]\n",
      "/home/ubuntu/anaconda3/envs/agentdriver/lib/python3.10/site-packages/gdown/__main__.py:140: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
      "  warnings.warn(\n",
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=14PWwG2oXkhOTY-hm8x3PiuKqZx7KKkSA\n",
      "From (redirected): https://drive.google.com/uc?id=14PWwG2oXkhOTY-hm8x3PiuKqZx7KKkSA&confirm=t&uuid=6c155574-6778-48ef-abe0-1aadf8d76f2a\n",
      "To: /home/ubuntu/Ollama/Agent-Driver/data/memory.zip\n",
      "100%|██████████████████████████████████████| 66.8M/66.8M [00:06<00:00, 11.1MB/s]\n",
      "/home/ubuntu/anaconda3/envs/agentdriver/lib/python3.10/site-packages/gdown/__main__.py:140: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
      "  warnings.warn(\n",
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1_XkcaZDuEXQ-8Tz4LeGNoDGlLHbPW6aE\n",
      "From (redirected): https://drive.google.com/uc?id=1_XkcaZDuEXQ-8Tz4LeGNoDGlLHbPW6aE&confirm=t&uuid=7fb4bc3c-6839-4913-860f-85d5fbc80aac\n",
      "To: /home/ubuntu/Ollama/Agent-Driver/data/metrics.zip\n",
      "100%|██████████████████████████████████████| 19.6M/19.6M [00:02<00:00, 9.47MB/s]\n"
     ]
    }
   ],
   "source": [
    "#!gdown --id 18vSSpL_52xVGEgK8vD_TcHFNLby0og0X -O ../../data/data.zip\n",
    "!gdown --id 1i-JXtbOsvXuKyT1sWKX9utjpRuo1isrW -O ../../data/finetune.zip\n",
    "!gdown --id 14PWwG2oXkhOTY-hm8x3PiuKqZx7KKkSA -O ../../data/memory.zip\n",
    "!gdown --id 1_XkcaZDuEXQ-8Tz4LeGNoDGlLHbPW6aE -O ../../data/metrics.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  ../../data/finetune.zip\n",
      "   creating: ../../data/finetune/\n",
      "  inflating: ../../data/finetune/data_samples_train.json  \n",
      "  inflating: ../../data/finetune/data_samples_val.json  \n",
      "Archive:  ../../data/memory.zip\n",
      "replace ../../data/memory/database.pkl? [y]es, [n]o, [A]ll, [N]one, [r]ename: ^C\n",
      "Archive:  ../../data/metrics.zip\n",
      "replace ../../data/metrics/uniad_gt_seg.pkl? [y]es, [n]o, [A]ll, [N]one, [r]ename: ^C\n"
     ]
    }
   ],
   "source": [
    "#!unzip ../../data/data.zip -d ../../data\n",
    "!unzip ../../data/finetune.zip -d ../../data\n",
    "!unzip ../../data/memory.zip -d ../../data\n",
    "!unzip ../../data/metrics.zip -d ../../data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**A Language Agent for Autonomous Driving**\n",
      "Role: You are the brain of an autonomous vehicle (a.k.a. ego-vehicle). In this step, you need to extract necessary information from the driving scenario. The information you extracted must be useful to the next-step motion planning. \n",
      "\n",
      "Necessary information might include the following:\n",
      "- Detections: The detected objects that you need to pay attention to.\n",
      "- Predictions: The estimated future motions of the detected objects. \n",
      "- Maps: Map information includes traffic lanes and road boundaries.\n",
      "- Occunpancy: Occupancy implies whether a location has been occupied by other objects.\n",
      "\n",
      "Task\n",
      "- You should think about what types of information (Detections, Predictions, Maps, Occupancy) you need to extract from the driving scenario.\n",
      "- Detections and Predictions are quite important for motion planning. You should call at least one of them if necessary.\n",
      "- Maps information are also important. You should pay more attention to road shoulder and lane divider information to your current ego-vehicle location.\n",
      "- I will guide you through the thinking process step by step.\n",
      "\n",
      "ego_prompts\n",
      "\n",
      "Sending to Ollama: {'model': 'llama3', 'messages': [{'role': 'system', 'content': '\\n**A Language Agent for Autonomous Driving**\\nRole: You are the brain of an autonomous vehicle (a.k.a. ego-vehicle). In this step, you need to extract necessary information from the driving scenario. The information you extracted must be useful to the next-step motion planning. \\n\\nNecessary information might include the following:\\n- Detections: The detected objects that you need to pay attention to.\\n- Predictions: The estimated future motions of the detected objects. \\n- Maps: Map information includes traffic lanes and road boundaries.\\n- Occunpancy: Occupancy implies whether a location has been occupied by other objects.\\n\\nTask\\n- You should think about what types of information (Detections, Predictions, Maps, Occupancy) you need to extract from the driving scenario.\\n- Detections and Predictions are quite important for motion planning. You should call at least one of them if necessary.\\n- Maps information are also important. You should pay more attention to road shoulder and lane divider information to your current ego-vehicle location.\\n- I will guide you through the thinking process step by step.\\n\\nego_prompts\\n'}, {'role': 'user', 'content': '\\nDo you need to perform detections from the driving scenario?\\nPlease answer YES or NO.\\n'}], 'options': {'temperature': 0.0}}\n",
      "{'role': 'assistant', 'content': ''}\n",
      "Sending to Ollama: {'model': 'llama3', 'messages': [{'role': 'system', 'content': '\\n**A Language Agent for Autonomous Driving**\\nRole: You are the brain of an autonomous vehicle (a.k.a. ego-vehicle). In this step, you need to extract necessary information from the driving scenario. The information you extracted must be useful to the next-step motion planning. \\n\\nNecessary information might include the following:\\n- Detections: The detected objects that you need to pay attention to.\\n- Predictions: The estimated future motions of the detected objects. \\n- Maps: Map information includes traffic lanes and road boundaries.\\n- Occunpancy: Occupancy implies whether a location has been occupied by other objects.\\n\\nTask\\n- You should think about what types of information (Detections, Predictions, Maps, Occupancy) you need to extract from the driving scenario.\\n- Detections and Predictions are quite important for motion planning. You should call at least one of them if necessary.\\n- Maps information are also important. You should pay more attention to road shoulder and lane divider information to your current ego-vehicle location.\\n- I will guide you through the thinking process step by step.\\n\\nego_prompts\\n'}, {'role': 'user', 'content': '\\nDo you need to perform detections from the driving scenario?\\nPlease answer YES or NO.\\n'}, {'role': 'assistant', 'content': ''}, {'role': 'user', 'content': '\\nDo you need to perform future trajectory predictions for the detected objects?\\nPlease answer YES or NO.\\n'}], 'options': {'temperature': 0.0}}\n",
      "{'role': 'assistant', 'content': ''}\n",
      "Sending to Ollama: {'model': 'llama3', 'messages': [{'role': 'system', 'content': '\\n**A Language Agent for Autonomous Driving**\\nRole: You are the brain of an autonomous vehicle (a.k.a. ego-vehicle). In this step, you need to extract necessary information from the driving scenario. The information you extracted must be useful to the next-step motion planning. \\n\\nNecessary information might include the following:\\n- Detections: The detected objects that you need to pay attention to.\\n- Predictions: The estimated future motions of the detected objects. \\n- Maps: Map information includes traffic lanes and road boundaries.\\n- Occunpancy: Occupancy implies whether a location has been occupied by other objects.\\n\\nTask\\n- You should think about what types of information (Detections, Predictions, Maps, Occupancy) you need to extract from the driving scenario.\\n- Detections and Predictions are quite important for motion planning. You should call at least one of them if necessary.\\n- Maps information are also important. You should pay more attention to road shoulder and lane divider information to your current ego-vehicle location.\\n- I will guide you through the thinking process step by step.\\n\\nego_prompts\\n'}, {'role': 'user', 'content': '\\nDo you need to perform detections from the driving scenario?\\nPlease answer YES or NO.\\n'}, {'role': 'assistant', 'content': ''}, {'role': 'user', 'content': '\\nDo you need to perform future trajectory predictions for the detected objects?\\nPlease answer YES or NO.\\n'}, {'role': 'assistant', 'content': ''}, {'role': 'user', 'content': '\\nDo you need to get occupancy information for this driving scenario?\\nPlease answer YES or NO.\\n'}], 'options': {'temperature': 0.0}}\n",
      "{'role': 'assistant', 'content': ''}\n",
      "Sending to Ollama: {'model': 'llama3', 'messages': [{'role': 'system', 'content': '\\n**A Language Agent for Autonomous Driving**\\nRole: You are the brain of an autonomous vehicle (a.k.a. ego-vehicle). In this step, you need to extract necessary information from the driving scenario. The information you extracted must be useful to the next-step motion planning. \\n\\nNecessary information might include the following:\\n- Detections: The detected objects that you need to pay attention to.\\n- Predictions: The estimated future motions of the detected objects. \\n- Maps: Map information includes traffic lanes and road boundaries.\\n- Occunpancy: Occupancy implies whether a location has been occupied by other objects.\\n\\nTask\\n- You should think about what types of information (Detections, Predictions, Maps, Occupancy) you need to extract from the driving scenario.\\n- Detections and Predictions are quite important for motion planning. You should call at least one of them if necessary.\\n- Maps information are also important. You should pay more attention to road shoulder and lane divider information to your current ego-vehicle location.\\n- I will guide you through the thinking process step by step.\\n\\nego_prompts\\n'}, {'role': 'user', 'content': '\\nDo you need to perform detections from the driving scenario?\\nPlease answer YES or NO.\\n'}, {'role': 'assistant', 'content': ''}, {'role': 'user', 'content': '\\nDo you need to perform future trajectory predictions for the detected objects?\\nPlease answer YES or NO.\\n'}, {'role': 'assistant', 'content': ''}, {'role': 'user', 'content': '\\nDo you need to get occupancy information for this driving scenario?\\nPlease answer YES or NO.\\n'}, {'role': 'assistant', 'content': ''}, {'role': 'user', 'content': '\\nDo you need to get map information for this driving scenario?\\nPlease answer YES or NO.\\n'}], 'options': {'temperature': 0.0}}\n",
      "{'role': 'assistant', 'content': ''}\n",
      "Working momory keys:  dict_keys(['token', 'ego_data'])\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'ego_states'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 24\u001b[0m\n\u001b[1;32m     15\u001b[0m language_agent \u001b[38;5;241m=\u001b[39m LanguageAgent(\n\u001b[1;32m     16\u001b[0m     data_path, split,\n\u001b[1;32m     17\u001b[0m     planner_model_name\u001b[38;5;241m=\u001b[39mFINETUNE_PLANNER_NAME,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     20\u001b[0m     backend\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mollama\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     21\u001b[0m )\n\u001b[1;32m     23\u001b[0m token \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0a0d6b8c2e884134a3b48df43d54c36a\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 24\u001b[0m \u001b[43mlanguage_agent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minference_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Ollama/Agent-Driver/agentdriver/unit_test/../../agentdriver/main/language_agent.py:147\u001b[0m, in \u001b[0;36mLanguageAgent.inference_single\u001b[0;34m(self, token, data_sample)\u001b[0m\n\u001b[1;32m    139\u001b[0m memory_agent \u001b[38;5;241m=\u001b[39m MemoryAgent(\n\u001b[1;32m    140\u001b[0m     data_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_path, \n\u001b[1;32m    141\u001b[0m     model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_name, \n\u001b[1;32m    142\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[1;32m    143\u001b[0m     backend\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackend\n\u001b[1;32m    144\u001b[0m )\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWorking momory keys: \u001b[39m\u001b[38;5;124m'\u001b[39m, working_memory\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[0;32m--> 147\u001b[0m commonsense_mem, experience_mem \u001b[38;5;241m=\u001b[39m \u001b[43mmemory_agent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mworking_memory\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    148\u001b[0m reasoning_agent \u001b[38;5;241m=\u001b[39m ReasoningAgent(\n\u001b[1;32m    149\u001b[0m     model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_name, \n\u001b[1;32m    150\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[1;32m    151\u001b[0m     backend\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackend\n\u001b[1;32m    152\u001b[0m )\n\u001b[1;32m    153\u001b[0m reasoning \u001b[38;5;241m=\u001b[39m reasoning_agent\u001b[38;5;241m.\u001b[39mrun(\n\u001b[1;32m    154\u001b[0m     perception_agent\u001b[38;5;241m.\u001b[39mdata_dict, \n\u001b[1;32m    155\u001b[0m     ego_prompts\u001b[38;5;241m+\u001b[39mperception_prompts, \n\u001b[1;32m    156\u001b[0m     working_memory, \n\u001b[1;32m    157\u001b[0m     use_cot_rules\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinetune_cot\n\u001b[1;32m    158\u001b[0m )\n",
      "File \u001b[0;32m~/Ollama/Agent-Driver/agentdriver/unit_test/../../agentdriver/llm_core/timeout.py:19\u001b[0m, in \u001b[0;36mtimeout.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     17\u001b[0m signal\u001b[38;5;241m.\u001b[39malarm(seconds)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 19\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;66;03m# Disable the alarm after the function call\u001b[39;00m\n\u001b[1;32m     22\u001b[0m     signal\u001b[38;5;241m.\u001b[39malarm(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/Ollama/Agent-Driver/agentdriver/unit_test/../../agentdriver/memory/memory_agent.py:33\u001b[0m, in \u001b[0;36mMemoryAgent.run\u001b[0;34m(self, working_memory)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;129m@timeout\u001b[39m(\u001b[38;5;241m60\u001b[39m)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m, working_memory):\n\u001b[1;32m     32\u001b[0m     common_sense_prompts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretrieve_common_sense_memory()\n\u001b[0;32m---> 33\u001b[0m     experience_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretrieve_experience_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mworking_memory\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m common_sense_prompts, experience_prompt\n",
      "File \u001b[0;32m~/Ollama/Agent-Driver/agentdriver/unit_test/../../agentdriver/memory/memory_agent.py:22\u001b[0m, in \u001b[0;36mMemoryAgent.retrieve_experience_memory\u001b[0;34m(self, working_memory)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mretrieve_experience_memory\u001b[39m(\u001b[38;5;28mself\u001b[39m, working_memory):\n\u001b[0;32m---> 22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexperience_memory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mworking_memory\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Ollama/Agent-Driver/agentdriver/unit_test/../../agentdriver/memory/experience_memory.py:180\u001b[0m, in \u001b[0;36mExperienceMemory.retrieve\u001b[0;34m(self, working_memory)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mretrieve\u001b[39m(\u001b[38;5;28mself\u001b[39m, working_memory):  \n\u001b[1;32m    178\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Retrieve the most similar past driving experiences with current working memory as input.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 180\u001b[0m     retrieved_scenes, confidence \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvector_retrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mworking_memory\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    181\u001b[0m     retrieved_mem_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgpt_retrieve(working_memory, retrieved_scenes, confidence)\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m retrieved_mem_prompt\n",
      "File \u001b[0;32m~/Ollama/Agent-Driver/agentdriver/unit_test/../../agentdriver/memory/experience_memory.py:99\u001b[0m, in \u001b[0;36mExperienceMemory.vector_retrieve\u001b[0;34m(self, working_memory)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mvector_retrieve\u001b[39m(\u001b[38;5;28mself\u001b[39m, working_memory):\n\u001b[1;32m     98\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\" Step-1 Vectorized Retrieval \"\"\"\u001b[39;00m        \n\u001b[0;32m---> 99\u001b[0m     querys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen_vector_keys\u001b[49m\u001b[43m(\u001b[49m\u001b[43mworking_memory\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mego_data\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m     top_k_indices, confidence \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_similarity(querys, working_memory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    102\u001b[0m     retrieved_scenes \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalues[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m top_k_indices]\u001b[38;5;241m.\u001b[39mcopy()\n",
      "File \u001b[0;32m~/Ollama/Agent-Driver/agentdriver/unit_test/../../agentdriver/memory/experience_memory.py:43\u001b[0m, in \u001b[0;36mExperienceMemory.gen_vector_keys\u001b[0;34m(self, data_dict)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mgen_vector_keys\u001b[39m(\u001b[38;5;28mself\u001b[39m, data_dict):\n\u001b[0;32m---> 43\u001b[0m     vx \u001b[38;5;241m=\u001b[39m \u001b[43mdata_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mego_states\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m0.5\u001b[39m\n\u001b[1;32m     44\u001b[0m     vy \u001b[38;5;241m=\u001b[39m data_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mego_states\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m0.5\u001b[39m\n\u001b[1;32m     45\u001b[0m     v_yaw \u001b[38;5;241m=\u001b[39m data_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mego_states\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m4\u001b[39m]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'ego_states'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append(\"../..\")\n",
    "\n",
    "from agentdriver.main.language_agent import LanguageAgent\n",
    "from agentdriver.llm_core.api_keys import OPENAI_ORG, OPENAI_API_KEY, FINETUNE_PLANNER_NAME\n",
    "\n",
    "import openai\n",
    "openai.organization = OPENAI_ORG\n",
    "openai.api_key = OPENAI_API_KEY\n",
    "\n",
    "data_path = Path('../../data/')\n",
    "split = 'val'\n",
    "# language_agent = LanguageAgent(data_path, split, planner_model_name=FINETUNE_PLANNER_NAME, verbose=True)\n",
    "language_agent = LanguageAgent(\n",
    "    data_path, split,\n",
    "    planner_model_name=FINETUNE_PLANNER_NAME,\n",
    "    verbose=True,\n",
    "    model_name=\"llama3\",   # or whatever Ollama model you want\n",
    "    backend=\"ollama\"\n",
    ")\n",
    "\n",
    "token = \"0a0d6b8c2e884134a3b48df43d54c36a\"\n",
    "language_agent.inference_single(token=token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
